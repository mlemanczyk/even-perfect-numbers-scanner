Windowed exponentiation and ProcessEightBitWindows adoption
- [EvenPerfectBitScanner.Benchmarks/Pow2OddPowerTableBenchmarks.cs:25] // TODO: We can calculate baseValue % modulus before loop and use it to increase ladderEntry calculation speed - we'll reuse the base for incremental calculations.
- [EvenPerfectBitScanner.Benchmarks/Pow2OddPowerTableBenchmarks.cs:574] // TODO: We can calculate baseValue % modulus before loop and use it to increase ladderEntry calculation speed - we'll reuse the base for incremental calculations.
- [EvenPerfectBitScanner.Benchmarks/Pow2OddPowerTableBenchmarks.cs:1116] // TODO: We can calculate baseValue % modulus before loop and use it to increase ladderEntry calculation speed - we'll reuse the base for incremental calculations.
- [EvenPerfectBitScanner.Benchmarks/Pow2OddPowerTableBenchmarks.cs:1789] // TODO: We can calculate baseValue % modulus before loop and use it to increase ladderEntry calculation speed - we'll reuse the base for incremental calculations.
- [done] [PerfectNumbers.Core/Cpu/MersenneNumberLucasLehmerCpuTester.cs:34] // TODO: Port this Lucas–Lehmer powmod to the ProcessEightBitWindows helper so the CPU fallback benefits from the same windowed ladder that halves runtime in the Pow2 benchmarks.
- [PerfectNumbers.Core/Cpu/MersenneNumberResidueCpuTester.cs:15] // TODO: Wire this residue scan into DivisorCycleCache so the mandatory cycle acceleration from the by-divisor benchmarks applies here instead of relying solely on ModResidueTracker powmods.
- [done] [PerfectNumbers.Core/CycleRemainderStepper.cs:72] // Overflow deltas now route through the cached wide-cycle reducer instead of using UInt128 modulo operations.
- [done] [PerfectNumbers.Core/ExponentRemainderStepper.cs:60] // Delta multipliers now reuse the windowed pow2 helper so incremental updates stay on the ProcessEightBitWindows ladder.
- [done] [PerfectNumbers.Core/ExponentRemainderStepper.cs:78] // Reload path already targets the windowed pow2 helper for consistent ProcessEightBitWindows timings.
- [done] [PerfectNumbers.Core/ExponentRemainderStepper.cs:89] // Unity checks reuse the windowed pow2 helper, keeping the delta path on the optimized ladder.
- [done] [PerfectNumbers.Core/Gpu/GpuContextPool.cs:10] // Pooled accelerators prewarm ProcessEightBitWindows kernels and upload divisor-cycle snapshots during construction.
- [done] [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:325] // TODO: Replace these Pow2Mod calls with the ProcessEightBitWindows helper when the shared windowed scalar implementation lands; benchmarks showed the windowed kernel trimming per-divisor runtime by ~2.4×.
- [done] [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:363] // TODO: Once the ProcessEightBitWindows helper is available, switch this order kernel to that faster Pow2Mod variant so cycle checks inherit the same gains observed in GpuPow2ModBenchmarks.
- [done] [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:481] // TODO: Swap Pow2Minus1Mod for the eight-bit window helper once the scalar version switches; benchmarks show the windowed variant cuts large-divisor scans from ~51 µs to ~21 µs.
- [done] [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:638] // TODO: Upgrade this pow2mod order kernel to the ProcessEightBitWindows helper once available so GPU residue scans avoid the single-bit ladder that benchmarks found to be 2.3× slower on large exponents.
- [done] [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:717] // TODO: Replace this Pow2Mod check with the ProcessEightBitWindows helper once Pow2Minus1Mod adopts it; residue order scans will then benefit from the same 2× speedup the benchmarked windowed kernel delivered.
- [done] [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1071] // TODO: Collapse this eight-step shift ladder into the ProcessEightBitWindows helper once it lands so we reuse the precomputed window residues instead of emitting `% modulus` after every shift.
- [done] [PerfectNumbers.Core/Gpu/MersenneNumberDivisorGpuTester.cs:149] // TODO: Swap Pow2Mod for the ProcessEightBitWindows helper once Pow2Minus1Mod adopts it; GpuPow2ModBenchmarks showed the windowed kernel cutting large divisors from ~51 µs to ~21 µs.
- [done] [PerfectNumbers.Core/Gpu/MersenneNumberDivisorGpuTester.cs:168] // TODO: Replace this trailing Pow2Mod call with the ProcessEightBitWindows variant once the shared windowed helper lands so mixed-radix decompositions benefit from the same GPU speedup.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:116] var kernel = GetBatchKernel(accelerator); // TODO: Switch this batch path to the ProcessEightBitWindows residue kernel once Lucas–Lehmer integrates the benchmarked windowed pow2 helper for small exponents.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:479] // TODO: Route these powmods through the ProcessEightBitWindows helper once it lands for GPU host-side fallbacks so the Lucas–Lehmer setup reuses the benchmarked windowed ladder.
- [PerfectNumbers.Core/Gpu/MersenneNumberOrderGpuTester.cs:25] }; // TODO: Migrate the Pow2Mod branch to the ProcessEightBitWindows order kernel once the shared helper replaces the single-bit ladder so GPU order scans match benchmark wins.
- [PerfectNumbers.Core/Gpu/MersenneNumberOrderGpuTester.cs:26] // TODO: Inline the IncrementalOrderKernel call once the ProcessEightBitWindows helper lands so this wrapper stops forwarding directly to the kernel and the hot path loses one indirection.
- [done] [PerfectNumbers.Core/Gpu/MersenneNumberResidueGpuTester.cs:30] var kernel = gpuLease.Pow2ModWindowedKernel; // TODO: Swap this to the ProcessEightBitWindows kernel once GpuUInt128.Pow2Minus1Mod adopts the shared windowed helper measured fastest in GpuPow2ModBenchmarks.
- [PerfectNumbers.Core/Gpu/MersenneNumberResidueGpuTester.cs:98] q += twoP * processed; // TODO: Swap this multiplication for the shared cycle stepping helper once residue cycles are mandatory so we can reuse the cached remainder ladder instead of performing UInt128 multiply operations.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:500] // TODO: Use the bitmask-based remainder helper here as well to remove `%` from the butterfly stage and align with the optimized kernels highlighted in the GPU pow2mod benchmarks.
- [PerfectNumbers.Core/Gpu/Pow2OddPowerTable.cs:18] // TODO: We can calculate baseValue % modulus before loop and use it to increase ladderEntry calculation speed - we'll reuse the base for incremental calculations.
- [PerfectNumbers.Core/Gpu/PrimeOrderGpuHeuristics.cs:1860] // TODO: We can calculate baseValue % modulus before loop and use it to increase ladderEntry calculation speed - we'll reuse the base for incremental calculations.
- [PerfectNumbers.Core/MersenneDivisorCycles.cs:1173] // TODO: Expose a GPU-first branch here so high divisors leverage the ProcessEightBitWindows kernel measured fastest in CycleLengthGpuVsCpuBenchmarks, returning the result without storing it in the shared cache.
- [PerfectNumbers.Core/MersenneNumberTester.cs:99] // TODO: Swap this fallback to the shared windowed order helper once CalculateOrder migrates to the ProcessEightBitWindows pipeline so warm-ups stop invoking the slower legacy powmod.
- [PerfectNumbers.Core/MersenneNumberTester.cs:189] // TODO: Replace this CalculateOrder call with the upcoming windowed helper so GPU warm-ups reuse the faster pow2 ladder measured in the CPU order benchmarks.
- [PerfectNumbers.Core/MersenneNumberTester.cs:328] // TODO: Swap this legacy kernel over to the ProcessEightBitWindows helper so GPU order scans share the eight-bit window pow2 implementation that beat the classic PowMod path in the GpuPow2Mod benchmarks.
- [PerfectNumbers.Core/MersenneResidueStepper.cs:20] // TODO: Replace these `%` reductions with the shared ProcessEightBitWindows helper so initialization benefits from the windowed pow2mod pipeline proven faster than the generic modulo path.
- [PerfectNumbers.Core/MersenneResidueStepper.cs:42] // TODO: Replace this repeated doubling with the shared ProcessEightBitWindows helper (or cycle-aware lookup) once the scalar pow2mod upgrade lands; benchmarking showed the windowed ladder slashes latency for large deltas compared to this linear loop.
- [PerfectNumbers.Core/MersenneResidueStepper.cs:57] // TODO: Once the scalar windowed pow2 helper lands, reroute this `%` path through it to avoid the slower multiply-and-mod sequence measured in the legacy benchmarks.
- [PerfectNumbers.Core/MersenneResidueStepper.cs:71] // TODO: Use the shared pow2mod remainder cache here so this `%` becomes a subtraction-based fold instead of the slower modulo operator for large moduli.
- [PerfectNumbers.Core/ModResidueTracker.cs:206] // TODO: Replace this PowMod128 call with the upcoming eight-bit window helper once the ProcessEightBitWindows scalar implementation lands so residue updates benefit from the ~2× win recorded in GpuPow2ModBenchmarks for large moduli.
- [PerfectNumbers.Core/ModResidueTracker.cs:263] // TODO: Consult MersenneDivisorCycles.Shared (or similar caches) here so we reuse precomputed cycle lengths instead of recomputing powmods for every divisor; the divisor-cycle benchmarks showed large wins once the cached orders were used across scans, and a miss should trigger the configured device to compute the cycle immediately without storing it back or requesting additional cache blocks.
- [PerfectNumbers.Core/PrimesGenerator.cs:78] // TODO: Reuse a single cached `candidateSquared` per iteration so we avoid issuing three separate 64-bit multiplications when populating the pow2 arrays here and in the last-one/last-seven branches below.
- [PerfectNumbers.Core/PrimeTester.cs:357] // TODO: Swap this handwritten binary GCD for the optimized helper measured in GpuUInt128BinaryGcdBenchmarks so CPU callers share the faster subtract-less ladder once the common implementation is promoted into PerfectNumbers.Core.
- [PerfectNumbers.Core/PrimeTester.cs:398] // TODO: Replace this inline GPU binary GCD with the kernel extracted from GpuUInt128BinaryGcdBenchmarks via GpuKernelPool so device callers reuse the fully unrolled ladder instead of this branchy fallback.
- [PerfectNumbers.Core/RationalHelper.cs:28] // TODO: Inline this floor conversion with the span-based ladder that dominated ResidueComputationBenchmarks so the CPU path skips the expensive ERational -> EInteger materialization when reducing large 2kp+1 divisors.
- [PerfectNumbers.Core/UInt128Extensions.cs:16] // TODO: Replace this hand-rolled binary GCD with the shared subtract-free ladder from GpuUInt128BinaryGcdBenchmarks so wide operands reuse the optimized helper instead of repeating the slower shift/subtract loop on both CPU and GPU paths.
- [PerfectNumbers.Core/UInt128Extensions.cs:389] // TODO: Switch this divisor-order powmod to the ProcessEightBitWindows helper so the cycle factoring loop benefits from the faster windowed pow2 ladder measured in CPU benchmarks.
- [PerfectNumbers.Core/ULongExtensions.cs:1022] // TODO: Port this scalar PowMod fallback to the ProcessEightBitWindows helper so CPU callers get the eight-bit window wins measured against the classic square-and-subtract implementation. Return 1 because 2^0 = 1
- [done] [PerfectNumbers.Core/ULongExtensions.cs:1129] // TODO: Replace this UInt128-cycle overload with the ProcessEightBitWindows helper so large-exponent CPU scans reuse the faster windowed pow2 ladder instead of the manual rotation loop measured to lag behind in benchmarks. Return 1 because 2^0 = 1
- [done] [PerfectNumbers.Core/ULongExtensions.cs:1187] // TODO: Migrate this UInt128 exponent overload to ProcessEightBitWindows so the large-cycle reductions drop the slow manual loop that underperforms the windowed pow2 helper in the Pow2 benchmark suite. Return 1 because 2^0 = 1

Divisor cycle infrastructure and reuse
- [EvenPerfectBitScanner/Program.cs:478] // TODO: Wire GenerateGpu to the unrolled-hex kernel that led the MersenneDivisorCycleLengthGpuBenchmarks once it lands.
- [EvenPerfectBitScanner/Program.cs:499] // TODO: Keep a single cached block loaded from disk and honor the configured device when computing ad-hoc cycles for divisors that fall outside that snapshot instead of queuing generation of additional blocks.
- [EvenPerfectBitScanner/Program.cs:1472] // TODO: Integrate the divisor-cycle cache here so the small-prime sweep reuses precomputed remainders instead of running MergeOrAppend for every candidate and missing the cycle-accelerated early exits.
- [PerfectNumbers.Core/Cpu/CpuConstants.cs:9] // TODO: Generate these masks at startup from the benchmarked Mod10 automaton tables so CPU filtering stays aligned with the optimized divisor-cycle residue steps.
- [PerfectNumbers.Core/Cpu/MersenneNumberIncrementalCpuTester.cs:30] // TODO: Replace this direct GetCycle call with DivisorCycleCache.Lookup so we reuse the single snapshot block and compute missing cycles on the configured device without queuing additional block generation.
- [PerfectNumbers.Core/Cpu/MersenneNumberOrderCpuTester.cs:17] // TODO: When this lookup misses the snapshot, invoke the configured device to compute the single required cycle on demand without persisting it so the CPU order path retains the cycle-stepping speedups while honoring the no-extra-cache constraint for large divisors.
- [PerfectNumbers.Core/Cpu/MersenneNumberResidueCpuTester.cs:17] // TODO: When a required cycle is missing from the snapshot, compute only that single cycle on the device selected by the current settings, skip persisting the result, and avoid scheduling additional cache blocks so we keep operating with the single shared block.
- [PerfectNumbers.Core/Cpu/MersenneNumberResidueCpuTester.cs:91] // TODO: Once cycle data is exposed for residue scans, consult the cached divisor cycle here (and below) so every qualifying q reuses precomputed lengths instead of recomputing via tracker.MergeOrAppend. cycle-based quick check for small q
- [PerfectNumbers.Core/CycleRemainderStepper.cs:28] // TODO: Inline this reset at the call sites so the hot loops reuse struct reinitialization measured fastest in MersenneDivisorCycleLengthGpuBenchmarks, avoiding the extra method call when scanners need to restart stepping.
- [PerfectNumbers.Core/CycleRemainderStepper.cs:42] // TODO: Swap this `%` for the shared divisor-cycle remainder helper so initialization reuses the cached cycle deltas benchmarked faster than on-the-fly modulo work, matching the MersenneDivisorCycleLengthGpuBenchmarks winner for both CPU and GPU call sites.
- [PerfectNumbers.Core/CycleRemainderStepper.cs:85] // TODO: Route this `%` through the shared divisor-cycle helper so repeated wrap-arounds avoid modulo operations and match the benchmarked fast path highlighted in MersenneDivisorCycleLengthGpuBenchmarks.
- [PerfectNumbers.Core/CycleRemainderStepper.cs:96] // TODO: Expose these fields directly once the residue scanners adopt the single-cycle helper so the hot path can read them without paying for an additional wrapper call per iteration.
- [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:244] // TODO: Plumb the small-cycles device buffer into all kernels that can benefit (some already accept it). Consider a compact type (byte/ushort) for memory footprint.
- [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:780] // lock (kernels) // TODO: Remove this lock by pre-uploading the immutable small-cycle snapshot during initialization; once no mutation happens at runtime, the pool must expose a simple reference without synchronization. {
- [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:787] var host = MersenneDivisorCycles.Shared.ExportSmallCyclesSnapshot(); // TODO: Preload this device buffer during startup and keep it immutable so we can delete the lock above in favor of the preloaded snapshot.
- [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:806] // lock (kernels) // TODO: Inline these small-prime uploads into startup initialization alongside the small-cycle snapshot so we can drop runtime locking and keep the GPU pool free of synchronization. {
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:345] // TODO: Move this Parallel.For to the shared low-overhead work scheduler once the NTT parameter generator integrates with the GPU-first pipeline so parameter scans reuse the same batching strategy measured fastest in the divisor-cycle benchmarks.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:398] // TODO: Reuse the divisor-cycle cache to factor phi via the precomputed small-prime windows once the lookup tables land so primitive root searches stop iterating over slow trial divisions.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:461] // TODO: Swap this `%` for the divisor-cycle aware Mod helper once the residue pre-checks expose it so primality filtering avoids slow modulo instructions in tight loops.
- [PerfectNumbers.Core/Gpu/MersenneNumberOrderGpuTester.cs:42] // TODO: Replace these modulo recomputations with the divisor-cycle stepping deltas captured in MersenneDivisorCycleLengthGpuBenchmarks so each batch advances via cached residues from the single snapshot block instead of recalculating mod 10/8/5/3 in every iteration.
- [PerfectNumbers.Core/Gpu/MersenneNumberResidueGpuTester.cs:11] // TODO: Integrate MersenneDivisorCycles.Shared to consult cycle lengths for small q (<= 4M) and fast-reject candidates without launching heavy kernels. Consider a device-visible constant buffer per-accelerator via GpuKernelPool to avoid host round-trips.
- [PerfectNumbers.Core/Gpu/MersenneNumberResidueGpuTester.cs:14] // TODO: When cycles are missing for larger q values, compute only the single required cycle on the device requested by the caller, skip queuing additional block generation, and keep the snapshot cache untouched.
- [PerfectNumbers.Core/Gpu/MersenneNumberResidueGpuTester.cs:48] // TODO: Replace the direct UInt128 multiply/add updates below with the residue-cycle stepping helper so that q advances reuse cached cycle increments rather than recomputing twoP multiples on every iteration.
- [PerfectNumbers.Core/KRangeFinder.cs:53] // TODO: Replace Parallel.For with the shared low-overhead scheduler highlighted in the divisor-cycle coordination benchmarks so alpha sweeps avoid the thread-pool setup costs observed here.
- [PerfectNumbers.Core/MersenneDivisorCycles.cs:260] // TODO: Replace this naive doubling fallback with the unrolled-hex generator from MersenneDivisorCycleLengthGpuBenchmarks so large divisors avoid the millions of iterations measured in the scalar loop.
- [PerfectNumbers.Core/MersenneDivisorCycles.cs:263] // TODO: Route this miss to an ephemeral single-cycle computation on the device selected by the current configuration (and skip persisting the result) when the divisor falls outside the in-memory snapshot so we respect the large-p memory limits and avoid extra cache locks.
- [PerfectNumbers.Core/MersenneDivisorCycles.cs:296] // TODO: Port this UInt128 path to the unrolled-hex cycle calculator so wide divisors stop relying on the slow shift-and-subtract loop measured in the GPU benchmarks.
- [PerfectNumbers.Core/MersenneDivisorCycles.cs:870] // TODO: Replace this modulo-based filter with the cached Mod3/Mod5/Mod7/Mod11 helpers so the CPU generator stops paying for `%` in the small-prime sieve and lines up with the divisor-cycle pipeline.
- [PerfectNumbers.Core/MersenneDivisorCycles.cs:878] // TODO: Migrate this generation path to the shared unrolled-hex calculator so the CPU generator matches the GPU benchmark leader for large divisors.
- [PerfectNumbers.Core/MersenneDivisorCycles.cs:957] // TODO: Swap these modulo checks for the cached Mod3/Mod5/Mod7/Mod11 helpers so range initialization avoids repeated `%` calls before the unrolled-hex pipeline takes over.
- [PerfectNumbers.Core/MersenneDivisorCycles.cs:1007] // TODO: Replace this modulo sieve with the cached Mod helpers once the divisor-cycle cache is mandatory so cycle enumeration can drop the slower `%` operations in this hot loop.
- [PerfectNumbers.Core/MersenneDivisorCycles.cs:1170] // TODO: Switch this scalar fallback to the unrolled-hex stepping sequence once the generator is shared with CPU callers; the benchmark shows the unrolled variant winning decisively for divisors >= 131,071.
- [PerfectNumbers.Core/MersenneNumberTester.cs:51] // TODO: Ensure Program passes useResidue correctly and that residue-vs-incremental-vs-LL selection respects CLI flags. Also consider injecting a shared cycles cache (MersenneDivisorCycles.Shared) to both CPU and GPU testers.
- [PerfectNumbers.Core/MersenneNumberTester.cs:74] // TODO: Replace these repeated Mod10/Mod8/Mod3/Mod5 calls with a residue automaton walk so order warmups reuse the optimized cycle stepping validated in the residue benchmarks.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:50] // TODO: Replace this dictionary with the divisor-cycle order cache once the benchmarks confirm the shared cache can stream snapshot results without extra locking.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:71] // TODO: Replace this lock with the lock-free order cache once the divisor-cycle snapshot exposes deterministic ordering for single-cycle computations; we measured heavy contention in the factor benchmarks when many threads warm the cache concurrently.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:119] // TODO: Integrate divisor-cycle data here so repeated order refinement uses cached cycle lengths when available and computes missing orders on the configured device without persisting them in the shared cache.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:296] // TODO: Rent the result array from ArrayPool once the factoring pipeline consumes spans so we can recycle buffers across large factorizations. lock (FactorCacheLock) { FactorCache[n] = result; }
- [PerfectNumbers.Core/MersenneResidueStepper.cs:45] // TODO: Once divisor cycles power the residue stepper, prefer stepping by cycle lengths instead of incrementing one bit at a time so large jumps stay on the mandatory cycle-accelerated path.
- [PerfectNumbers.Core/MersenneResidueStepper.cs:64] // TODO: Switch to a cached divisor-cycle remainder instead of computing a fresh modular inverse for every backward jump; the divisor cycle benchmarks demonstrated significant savings once shared cycle data was used.
- [PerfectNumbers.Core/MersenneResidueStepper.cs:102] // TODO: Replace this division-heavy Euclidean loop with the binary modular inverse that reuses divisor cycles so we drop repeated `/` and `%` operations during backtracking steps.
- [PerfectNumbers.Core/ModResidueTracker.cs:189] // TODO: Replace this subtraction loop with the divisor-cycle stepping helper so identity residues reuse cached cycle deltas, computing the missing cycle on the configured device without caching the result or scheduling extra blocks when the snapshot lacks the divisor.
- [PerfectNumbers.Core/ModResidueTracker.cs:252] // TODO: Swap this modulo fallback for the shared divisor-cycle remainder helper so 64-bit residues reuse cached cycles instead of recomputing `%` on every lookup.
- [PerfectNumbers.Core/ModuloAutomata.cs:78] // TODO: Once divisor cycle lookups feed residue scanning, reuse the cached cycle length here to skip directly to the next admissible q instead of iterating through every residue update.
- [PerfectNumbers.Core/ModuloAutomata.cs:126] // TODO: Precompute the initial residues via the divisor-cycle cache so this setup phase reuses the GPU-generated cycle data instead of issuing on-the-fly modulo operations.
- [PerfectNumbers.Core/ModuloAutomata.cs:183] // TODO: Once divisor cycles are mandatory, reuse the cached cycle offset per modulus to skip directly to valid candidates so these per-step additions disappear from the hot path.
- [PerfectNumbers.Core/PerfectNumberConstants.cs:12] // TODO: Load these limits from the benchmark-driven configuration so CPU and GPU scans stay aligned with the optimal divisor-cycle datasets we generate offline.
- [PerfectNumbers.Core/PowerCache.cs:76] // TODO: Move the high-precision branch to the benchmark project once production switches to the divisor-cycle aware cache; maintaining this BigInteger multiply path in the hot pipeline keeps the slower code on the CPU scan.
- [PerfectNumbers.Core/PrimeCache.cs:96] // TODO: Apply the Mod6 stride scheduler here as well so on-demand extensions mirror the Mod6ComparisonBenchmarks winner when pulling primes for divisor-cycle generation.
- [PerfectNumbers.Core/PrimeTester.cs:82] // TODO: Route this small-prime filtering through the shared divisor-cycle cache once PrimeTester can consult it directly; the cached cycles avoid the repeated `%` work that slows these hot loops when sieving hundreds of millions of candidates.
- [PerfectNumbers.Core/PrimeTester.cs:309] // TODO: Route this modulo through the shared divisor-cycle cache once exposed to GPU kernels so batched sieves avoid per-prime `%` operations that profiling showed expensive.
- [PerfectNumbers.Core/PrimeTester.cs:322] // TODO: Replace this on-the-fly GCD probe with the cached factor table derived from ResidueComputationBenchmarks so divisor-cycle metadata can short-circuit the test instead of recomputing binary GCD for every candidate.
- [PerfectNumbers.Core/PrimeTester.cs:331] // TODO: Route this batch helper through the shared GPU kernel pool from GpuUInt128BinaryGcdBenchmarks so we reuse cached kernels, pinned host buffers, and divisor-cycle staging instead of allocating new device buffers per call.
- [PerfectNumbers.Core/RleBlacklist.cs:144] // TODO: Cache BuildRleKey results per divisor cycle bucket so residue scans reuse the normalized strings; recomputing the key for every candidate shows up heavily in the RLE blacklist profile once we cross the 138M threshold.
- [PerfectNumbers.Core/UInt128Extensions.cs:373] // TODO: If the cache lacks this cycle, immediately schedule the configured device (GPU by default) to compute it on the fly and skip inserting it into the cache so wide-order factoring can still leverage cycle stepping without breaching the memory cap or introducing extra synchronization.
- [PerfectNumbers.Core/UInt128Extensions.cs:433] // TODO: Replace this direct `%` test with the shared divisor-cycle filter once the UInt128 path is wired into the cached cycle tables so wide candidates skip the slow modulo checks during primality pre-filtering.
- [PerfectNumbers.Core/UInt128Extensions.cs:477] // TODO: Inline the lookup-based Mod7 reducer validated in the residue benchmarks so this helper stops relying on `% 7` in hot loops and mirrors the optimized CPU cycle filters.
- [PerfectNumbers.Core/ULongExtensions.cs:34] // TODO: When the shared cycle snapshot cannot serve this divisor, trigger an on-demand GPU computation (respecting the configured device) without promoting the result into the cache so the order calculator still benefits from cycle stepping while keeping the single-block memory plan intact.
- [PerfectNumbers.Core/ULongExtensions.cs:47] // TODO: Replace this `%` driven factor peeling with the divisor-cycle aware factoring helper so large orders reuse the cached remainders highlighted in the latest divisor-cycle benchmarks instead of recomputing slow modulo checks.
- [PerfectNumbers.Core/ULongExtensions.cs:125] // TODO: Swap this modulo check for the shared small-prime cycle filter once the divisor-cycle cache is mandatory, matching the PrimeTester improvements noted in the CPU sieve benchmarks.

Residue and modulo helper optimizations
- [EvenPerfectBitScanner/Program.cs:174] // TODO: Swap this TryParse for the zero-allocation Utf8Parser helper to keep residue CLI option parsing in the fast path.
- [PerfectNumbers.Core/Cpu/CpuConstants.cs:11] // TODO: Fold in the Mod6ComparisonBenchmarks stride table here so callers can merge the mod 6 skips with these masks and avoid redundant residue work in the CPU hot loop.
- [PerfectNumbers.Core/Cpu/MersenneNumberIncrementalCpuTester.cs:43] // TODO: Swap these Mod3/Mod5 zero checks to the cached residue tables once the automaton exposes the benchmarked lookup-based helpers so CPU scans avoid runtime modulo instructions.
- [PerfectNumbers.Core/Cpu/MersenneNumberLucasLehmerCpuTester.cs:11] // TODO: Replace these `%` checks with Mod3/Mod5/Mod7/Mod11 helpers once Lucas–Lehmer CPU filtering shares the benchmarked bitmask implementations and avoids slow modulo instructions in the hot path.
- [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:252] // TODO: Replace these `%` computations with the precomputed Mod3/Mod5 tables so GPU kernels reuse cached residues instead of performing modulo operations that the benchmarks showed slower on wide sweeps.
- [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:271] // TODO: Swap these `%` filters to the shared Mod helpers so the residue automaton matches the benchmarked bitmask-based implementation for GPU workloads.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:604] // TODO: Replace this scalar binary GCD with the branchless reduction from GpuUInt128BinaryGcdBenchmarks so CPU fallbacks stay aligned with the GPU kernel performance when resolving large divisor residues.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:57] // TODO: Replace this `% 3` guard with ULongExtensions.Mod3 once GPU LL filtering reuses the benchmarked bitmask helper.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:427] // TODO: Replace these `%` factor checks with the shared Mod helpers (Mod3/Mod5/etc.) once the GPU pre-filter adopts the benchmarked bitmask operations to avoid slow modulo instructions.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:407] // TODO(NTT-OPT): Mirror the forward stage-wise design for the inverse transform. Precompute inverse twiddles and a normalization factor (n^-1 mod m). Launch one short kernel per stage to avoid TDR.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:424] // TODO: Replace this `%` with a bitmask when `half` is a power of two so the stage index math matches the faster residue strategy from the GPU residue benchmarks.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:551] // TODO: Apply the bitmask remainder helper to this stage too so every butterfly path drops the slower `%` operation.
- [PerfectNumbers.Core/MersenneNumberTester.cs:117] // TODO: Reuse the same residue-automaton fast path here so the GPU warmup staging avoids `%` and branches the benchmarks showed slower than the tracked stepping helper.
- [PerfectNumbers.Core/MersenneNumberTester.cs:219] // TODO: Replace this `% 3` check with ULongExtensions.Mod3 to align the early rejection with the benchmarked bitmask helper instead of generic modulo for CPU workloads.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:39] // TODO: Replace this `%` with the benchmarked Math.DivRem fast path so the hot filter avoids 128-bit division when evaluating candidates.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:98] // TODO: Swap this `%`/`/` loop for the Math.DivRem-based reducer highlighted in the order benchmarks so we avoid repeated 128-bit division while trimming the order.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:439] t = a % b; // TODO: Replace this with the binary GCD helper used in the gcd benchmarks so we avoid slow modulo operations while factoring large composites.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:586] t = a % b; // TODO: Replace with the binary GCD helper once the UInt128 benchmarks land so we remove the slow `%` from the wide gcd loop.
- [PerfectNumbers.Core/MersenneResidueStepper.cs:26] // TODO: Inline this accessor into callers so the hot residue path reads the backing fields directly instead of jumping through a wrapper method every iteration.
- [PerfectNumbers.Core/MersenneResidueStepper.cs:80] // TODO: Collapse this wrapper once callers can invoke Step directly; the extra indirection shows up in the profiler when residue adjustments happen per candidate.
- [PerfectNumbers.Core/ModuloAutomata.cs:45] // TODO: Remove this wrapper and expose the field directly once callers switch to struct-based residue tracking so we can trim another layer from the tight scanning loop.
- [PerfectNumbers.Core/ModuloAutomata.cs:121] // TODO: Replace the `% 10` computation with the Mod10 helper once Ending7Automaton plugs into the shared residue tables so we avoid generic divisions while initializing the step cache.
- [PerfectNumbers.Core/ModuloAutomata.cs:133] // TODO: Swap this `%` check for the binary-gcd aware reducer once the automaton wiring shares the branchless Mod helpers benchmarked faster than repeated modulo instructions.
- [PerfectNumbers.Core/ModuloAutomata.cs:140] // TODO: Inline this getter into the automaton consumers so residue scans can read the backing field without paying the delegate call overhead highlighted in the profiler snapshots.
- [PerfectNumbers.Core/ModuloAutomata.cs:155] // TODO: Pre-register every modulus routed through the automaton so the fallback `%` path disappears; the benchmarked residue trackers beat this division-heavy branch when scanning millions of values.
- [PerfectNumbers.Core/ModuloAutomata.cs:201] // TODO: Switch this to the binary GCD helper used in GpuUInt128BinaryGcdBenchmarks so we avoid `%` inside the residue automaton setup.
- [PerfectNumbers.Core/PrimesGenerator.cs:63] // TODO: Replace this trial-division `%` with the sieve-based generator that avoids per-candidate modulo work so building the small-prime tables stops dominating startup time for large scans.
- [PerfectNumbers.Core/PrimesGenerator.cs:104] // TODO: Switch this increment to the Mod6 stepping table identified in the Mod6ComparisonBenchmarks so we skip the composite residues without relying on per-iteration `%` checks.
- [PerfectNumbers.Core/PrimesGenerator.cs:113] // TODO: Swap the `% 10` usage for ULongExtensions.Mod10 so the hot classification path reuses the benchmarked residue helper instead of repeated divisions.
- [PerfectNumbers.Core/PrimesGenerator.cs:125] // TODO: Route this `% 10` classification through ULongExtensions.Mod10 to match the faster residue helper used elsewhere in the scanner.
- [PerfectNumbers.Core/PrimeTester.cs:60] // TODO: Replace this modulo check with ULongExtensions.Mod5 so the CPU hot path reuses the benchmarked helper instead of `%` when sieving large prime ranges.
- [PerfectNumbers.Core/PrimeTester.cs:281] // TODO: Replace the `% 5` branch with the GPU Mod5 helper once the sieve kernel can reuse the benchmarked bitmask reduction instead of modulo on every candidate.
- [PerfectNumbers.Core/RationalHelper.cs:39] // TODO: Replace this ceiling branch with the branchless adjustment measured fastest in ResidueComputationBenchmarks so large-divisor scans avoid repeated ERational conversions on the CPU hot path.
- [PerfectNumbers.Core/RleBlacklist.cs:154] // TODO: Replace this per-bit loop with the lookup-table driven builder validated in the ModResidueTracker benchmarks; it amortizes the run detection across 8-bit blocks and cuts the blacklist check latency roughly in half on large batches. Build RLE from MSB to LSB runs.
- [PerfectNumbers.Core/UInt128Extensions.cs:448] // TODO: Fold these reductions into the multiply-high trick captured in Mod3BenchmarkResults so the UInt128 modulo helpers avoid `%` altogether and align with the faster CPU/GPU residue filters.
- [PerfectNumbers.Core/UInt128Extensions.cs:457] // TODO: Replace the `% 5` operations with the precomputed multiply-high constants from the Mod5 benchmarks so the UInt128 path matches the 64-bit helpers without extra modulo instructions.
- [PerfectNumbers.Core/UInt128Extensions.cs:501] // TODO: Precompute the 2^64 ≡ 6 folding constants once so this path stops recomputing `% 10` on each half and instead uses the span-based lookup captured in Mod10BenchmarkResults.
- [PerfectNumbers.Core/UInt128Extensions.cs:524] // TODO: Collapse this Mod10 switch into the shared lookup table from the CLI benchmarks so we avoid the nested switch expressions once the pooled residue tables become available.
- [PerfectNumbers.Core/UInt128Numbers.cs:16] // TODO: Populate this table from the UInt128 residue benchmarks so GPU kernels can pull precomputed constants without re-deriving them at runtime.
- [PerfectNumbers.Core/UInt128Numbers.cs:18] // TODO: Extend the table with cached Mod3/Mod5 folding constants so both CPU and GPU residue helpers can reuse the benchmarked multiply-high reductions without recomputing them per call.
- [PerfectNumbers.Core/UIntExtensions.cs:9] // TODO: Only add Mod7/Mod11 lookup helpers once we have a variant that beats the `%` baseline (current prototypes lose per Mod7/Mod11 benchmarks). the small-prime sieves.
- [PerfectNumbers.Core/ULongExtensions.cs:1266] // TODO: Replace this `% modulusCandidate` with the cached residue helper derived from Mod10_8_5_3Benchmarks so CRT composition avoids repeated modulo divisions when combining residues for large divisor sets.
- [PerfectNumbers.Core/ULongExtensions.cs:1275] // TODO: Swap this final `% modulus` with the pooled remainder cache so the CRT result write-back avoids one more division, aligning with the optimizations captured in Mod10_8_5_3Benchmarks.

Register pressure and variable reuse
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1350] // TODO: Reuse variables to reduce register pressure.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1364] // TODO: Reuse variables to reduce register pressure.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1385] // TODO: Reuse variables to reduce register pressure. Compute (H*2^64 + L)^2 = H^2*2^128 + 2*H*L*2^64 + L^2
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1467] // TODO: Reuse variables to reduce register pressure.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1564] // TODO: Reuse variables to reduce register pressure. t = aR * bR (128-bit)
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1592] // TODO: Reuse variables to reduce register pressure.

GPU kernel and accelerator tuning
- [EvenPerfectBitScanner.Benchmarks/MulHighBenchmarks.cs:40] // TODO: Replace ULongExtensions.MulHigh with the UInt128-based helper on CPU paths; it was 11–14× faster in the latest benchmarks while the GPU-specific variant remains available separately.
- [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:315] // TODO: q will be lowered by GpuUInt128.One here. Is this expected? Maybe that's why we have issues with the incremental kernel? Review other places where we use GpuUInt128 operators.
- [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:760] private static readonly ConcurrentDictionary<Accelerator, KernelContainer> KernelCache = new(); // TODO: Replace this concurrent map with a simple accelerator-indexed lookup once kernel launchers are prewarmed during startup so we can drop the thread-safe wrapper entirely.
- [PerfectNumbers.Core/Gpu/GpuPrimeWorkLimiter.cs:11] // TODO: Inline the pooled limiter guard from the GpuLimiterThroughputBenchmarks so prime-sieve GPU jobs stop allocating Releaser instances on every acquisition.
- [PerfectNumbers.Core/Gpu/GpuPrimeWorkLimiter.cs:32] // TODO: Switch to a shared limiter implementation with GpuWorkLimiter so we can coordinate CPU/GPU prime work using the same semaphore pool without reallocating per adjustment.
- [PerfectNumbers.Core/Gpu/GpuWorkLimiter.cs:11] // TODO: Replace the per-call Releaser allocation with the pooled struct-based guard from the GpuLimiterThroughputBenchmarks so limiter acquisition does not allocate when we enter the GPU scanning hot path.
- [PerfectNumbers.Core/Gpu/GpuWorkLimiter.cs:36] // TODO: Consolidate with GpuPrimeWorkLimiter so both limiters share a pooled SemaphoreSlim and avoid rebuilding the limiter state whenever limits are adjusted from the CLI.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:46] // TODO: Inline this wrapper once callers request IsPrime directly so the Lucas–Lehmer fast path avoids an extra method frame; the LucasLehmerGpuBenchmarks showed the delegate hop shaving measurable time off tight reload loops when removed.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:85] var modulus = new GpuUInt128(((UInt128)1 << (int)exponent) - 1UL); // TODO: Cache these Mersenne moduli per exponent so LL GPU runs skip rebuilding them every launch.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:126] // TODO: Replace this per-exponent shift with a small shared table (one entry per supported exponent < 128) so Lucas–Lehmer batch runs reuse cached GpuUInt128 moduli instead of rebuilding them for every request.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:190] // TODO(LL-SLICE): Slice Lucas–Lehmer iterations into short batches to avoid long-running kernels and TDR. Example: process 8–64 iterations per slice, with synchronization between slices. Combine this with stage-wise NTT to ensure each kernel stays < ~0.5–1.0s.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:307] // TODO: Switch these ulong.Parse calls to the Utf8Parser-based fast-path once we expose a zero-allocation reader for persisted kernel parameters.
- [PerfectNumbers.Core/Gpu/MersenneNumberOrderGpuTester.cs:28] var foundBuffer = accelerator.Allocate1D<int>(1); // TODO: Replace this allocation with the pooled device buffer from GpuOrderKernelBenchmarks so repeated scans reuse the pinned staging memory instead of allocating per run.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:32] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, ArrayView<GpuUInt128>, GpuUInt128>> MulKernelCache = new(); // TODO: Replace this concurrent cache with the prewarmed accelerator-indexed tables from GpuModularArithmeticBenchmarks so kernel launches avoid dictionary lookups once the kernels are baked during startup.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:33] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, int, int, int, ArrayView<GpuUInt128>, GpuUInt128>> StageKernelCache = new(); // TODO: Same as above – materialize staged kernels during startup so we can drop ConcurrentDictionary usage entirely per the benchmark guidance.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:34] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, GpuUInt128, GpuUInt128>> ScaleKernelCache = new(); // TODO: Promote to the static kernel table initialized alongside the GpuModularArithmeticBenchmarks fast path.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:40] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, ulong, ulong>> FromMont64KernelCache = new(); // TODO: As above – drop ConcurrentDictionary once startup prewarms every kernel.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:41] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, ulong, ulong>> SquareMont64KernelCache = new(); // TODO: Inline into the static kernel table established during initialization.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:44] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, ArrayView<GpuUInt128>, int, GpuUInt128, GpuUInt128>> ForwardKernelCache = new(); // TODO: Warm these forward kernels during initialization and store them in a plain array so the LucasLehmerGpuBenchmarks launch costs disappear.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:46] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, ArrayView<GpuUInt128>, int, GpuUInt128, GpuUInt128, GpuUInt128>> InverseKernelCache = new(); // TODO: Same as above – replace with startup-prepared tables per LucasLehmerGpuBenchmarks guidance.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:48] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, int>> BitReverseKernelCache = new(); // TODO: Precompute bit-reversal kernels during boot so we can remove the concurrent lookup overhead flagged in the benchmarks.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:235] // TODO(NTT-OPT): Add a TwiddleCache for stage-wise Cooley–Tukey NTT. Design: - Keyed by (Length, Modulus, PrimitiveRoot) per Accelerator. - Store one GPU buffer with all stage twiddles, or per-stage slices. - Expose accessors to retrieve per-stage views used by butterfly kernels.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:356] // TODO(NTT-OPT): Remove per-accelerator twiddle buffers once added.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:398] // TODO(NTT-OPT): Introduce stage-wise Cooley–Tukey kernels and a per-(accelerator,length,modulus,root) twiddle cache. The plan: - Precompute twiddle factors for all stages once and cache on GPU. - Replace the current O(n^2) ForwardKernel with O(n log n) butterflies. - Provide accessors like GetStageKernel(...) and GetTwiddleBuffer(...).
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:420] // TODO(NTT-OPT): Consider shared-memory tiling to improve locality: load a block of size `len` into shared memory, do butterflies, write back. Requires explicit grouped kernels and chosen group size.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:772] // TODO(NTT-OPT): Replace this O(n^2) inverse with stage-wise butterflies using precomputed inverse twiddles. Apply final scaling by nInv after all stages, preferably in a small dedicated kernel.
- [PerfectNumbers.Core/PrimeTester.cs:28] // TODO: Inline the single-value GPU sieve fast path from GpuModularArithmeticBenchmarks so this wrapper can skip stackalloc buffers and reuse the pinned upload span the benchmark identified as fastest.
- [PerfectNumbers.Core/PrimeTester.cs:121] // TODO: Replace this ad-hoc ArrayPool buffer with the pinned span cache from PrimeSieveGpuBenchmarks so batch uploads reuse preallocated GPU-friendly memory and avoid the extra copy before every kernel launch.
- [PerfectNumbers.Core/PrimeTester.cs:164] // TODO: Replace this ConcurrentBag with the lock-free ring buffer variant validated in GpuModularArithmeticBenchmarks so renting scratch buffers stops contending on the bag's internal locks when thousands of GPU batches execute per second.
- [PerfectNumbers.Core/PrimeTester.cs:237] // TODO: Prewarm this per-accelerator cache during startup (and reuse a simple array keyed by accelerator index) once the kernel pool exposes deterministic ordering; the Lazy wrappers showed measurable overhead in the GpuModularArithmeticBenchmarks hot path.
- [PerfectNumbers.Core/TextFileWriter.cs:80] // TODO: Buffer binary writes with the same ArrayPool-backed accumulator planned for text so GPU result dumps can batch disk flushes instead of flushing on every invocation.
- [PerfectNumbers.Core/ULongExtensions.cs:242] // TODO: Investigate replacing this manual decomposition with the UInt128-based implementation for CPU callers; the latest benchmarks show the intrinsic path is an order of magnitude faster, while GPU code can keep using GpuUInt128.MulHigh.

Resource pooling, caching, and allocation reduction
- [EvenPerfectBitScanner.ResultsParser/Program.cs:234] // TODO: Replace this vanilla StreamReader with a pooled FileStreamOptions + ArrayPool-backed reader so large reloads reuse the benchmarked zero-allocation buffered pipeline instead of allocating new decoder buffers per run.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:250] // TODO: Stream candidates through a pooled buffer pipeline so we can filter and dispatch without allocating the entire list upfront; the benchmarks showed that retaining every row simultaneously increases GC pressure during large result replays.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:333] // TODO: Switch this path composition to Span<char>-based stack buffers once we expose the pooled formatter helpers so output names avoid intermediate strings for directory/file splits.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:349] // TODO: Switch WriteCsv to a span-based Utf8Formatter pipeline backed by ArrayPool-rented buffers so appends reuse the zero-allocation formatting path highlighted in the results writer benchmarks.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:358] // TODO: Replace this per-entry WriteLine with a batched chunk writer that reuses pooled StringBuilder instances so large files avoid the per-line flush overhead observed in the chunked writer benchmarks.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:380] // TODO: Replace the Task-based scheduling with the pooled work queue used in the scanner so candidate parsing leverages the same low-overhead thread handoff profiled as fastest in the CLI benchmarks.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:388] // TODO: Rent this line buffer from ArrayPool<string> to eliminate repeated allocations when scanning large result files.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:452] // TODO: Rent this full-capacity buffer from ArrayPool<string> and reuse it between reads instead of allocating a fresh array each time steady-state batches are dispatched.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:458] // TODO: Rent the partial chunk buffer from ArrayPool<string> instead of allocating a fresh array for every remainder dispatch.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:499] // TODO: Rent the candidate array from ArrayPool<CandidateResult> so chunk processing remains allocation-free for hot reload paths.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:531] // TODO: Replace this copy with an in-place filtering pipeline that reuses pooled buffers; duplicating the list doubled working-set usage in the reload benchmarks when processing multi-million-entry snapshots.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:578] // TODO: Replace this Dictionary allocation with the pooled span-based frequency map once the ValueListBuilder helper lands so raw prime reconstruction stops allocating temporary hash tables for every reload.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:593] // TODO: Rent the raw prime accumulator from ArrayPool<List<CandidateResult>> or migrate to a pooled struct builder so large reloads avoid repeated List allocations along this path.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:661] // TODO: Rent these segment arrays from ArrayPool<List<CandidateResult>> so parallel splitting avoids allocating new jagged arrays on every reload; the pooled work queue already used by the scanner benchmarks faster.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:666] // TODO: Rent the tasks array from ArrayPool<Task> so repeated reloads avoid allocating new scheduling buffers; the pooled queue used by the scanner benchmarks noticeably faster.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:690] // TODO: Replace Task.Run with the pooled work queue from the scanner so scheduling overhead matches the benchmarked fast path instead of spawning transient threads.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:696] // TODO: Rent these per-worker lists from a dedicated pool so parallel splits reuse pre-sized buffers; allocating new List instances showed up heavily in the multi- threaded reload benchmarks once files crossed tens of millions of entries.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:721] // TODO: Promote these final result lists to pooled builders so we can reuse the buffers across reloads; the pooled aggregators from the scanner benchmarks avoid repeated large allocations during long parsing sessions.
- [EvenPerfectBitScanner/Program.cs:503] // TODO: Stop reloading the full snapshot once the ad-hoc path streams results straight from the configured device without persisting them, so memory stays bounded while preserving the single cached block strategy.
- [EvenPerfectBitScanner/Program.cs:656] // TODO: Replace this File.WriteAllText call with the pooled TextFileWriter pipeline once the scanner's output flush adopts the benchmarked buffered writer so we avoid allocating the entire header string and opening the file twice per run.
- [EvenPerfectBitScanner/Program.cs:677] // TODO: Rent this filter buffer from ArrayPool<ulong> and reuse it across reload batches so we do not allocate fresh arrays while replaying large result filters.
- [EvenPerfectBitScanner/Program.cs:765] // TODO: Rent this buffer from ArrayPool once the pooled prime-block allocator lands so worker threads stop allocating fresh arrays for every reservation.
- [EvenPerfectBitScanner/Program.cs:1047] // TODO: Replace this string interpolation with the pooled ValueStringBuilder pipeline from the results-writer benchmarks so filename generation reuses the zero-allocation formatter once we consolidate output paths.
- [EvenPerfectBitScanner/Program.cs:1070] // TODO: Inline the fast-path transform here once we collapse the delegate* indirection so block reservations stop paying the extra jump highlighted in the prime-stepping benchmarks.
- [EvenPerfectBitScanner/Program.cs:1210] // TODO: Replace these per-flush FileStream/StreamWriter allocations with a pooled TextFileWriter-style helper so batched result writes keep the hot path on the benchmarked persistent-handle pipeline.
- [EvenPerfectBitScanner/Program.cs:1528] // TODO: Replace this byte-by-byte scan with the lookup-table based statistics collector validated in the BitStats benchmarks so zero runs leverage cached results instead of recomputing per bit.
- [PerfectNumbers.Core/AlphaCache.cs:11] // TODO: Replace this dictionary lookup with a lock-free cache that reuses pooled AlphaValues so alpha requests avoid redoing the expensive PowerCache.Get computations across threads.
- [PerfectNumbers.Core/AlphaCache.cs:26] // TODO: Return pooled AlphaValues instances to a shared cache before clearing so repeated warm-ups do not thrash the allocator when alpha tables refresh during large divisor scans.
- [PerfectNumbers.Core/AlphaCalculations.cs:11] // TODO: Reuse pooled numerator/denominator buffers here so the alphaP computation stops allocating intermediate EInteger instances every invocation.
- [PerfectNumbers.Core/AlphaCalculations.cs:20] // TODO: Reuse pooled ERational builders here so repeated factor computations pull precomputed numerator/denominator tuples instead of re-executing PowerCache chains each time, matching the fastest alpha benchmarks.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:300] // TODO: Replace int.TryParse with the span-based Utf8Parser helper when loading cached parameters so we avoid culture-aware parsing in this hot startup loop.
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:327] // TODO: Replace StreamWriter with the pooled TextFileWriter pipeline so persisting NTT parameters reuses the zero-allocation buffered writes highlighted in the scanner I/O benchmarks instead of allocating a new encoder per append.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:333] // TODO(NTT-OPT): Clear TwiddleCache once introduced.
- [PerfectNumbers.Core/KRangeFinder.cs:13] // TODO: Pull alphaP from AlphaCache (or a pooled rational builder) so this loop reuses the benchmarked cached values instead of recomputing AlphaCalculations.ComputeAlphaP for every k.
- [PerfectNumbers.Core/KRangeFinder.cs:16] // TODO: Switch this multiply to the pooled ERational span helpers identified in the scanner's alpha rational benchmarks so we avoid allocating new big-number intermediates per iteration.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:84] // TODO: Pull these factor arrays from ArrayPool once the factoring helpers adopt the pooled buffers measured faster in FactorizationBenchmarks.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:256] var queue = new Queue<ulong>(); // TODO: Replace this Queue with the pooled stack from the FactorizationBenchmarks fast path so factoring large composites avoids per-node allocations.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:315] var queue = new Queue<UInt128>(); // TODO: Replace this with the pooled UInt128 stack once the wide-factorization benchmarks finalize the faster span based traversal.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:354] var result = new UInt128[dict.Count]; // TODO: Rent this array from ArrayPool<UInt128> so wide-factorization batches stop allocating fresh buffers per candidate.
- [PerfectNumbers.Core/PowerCache.cs:27] // TODO: Replace Array.Resize with a pooled copy so we reuse previously rented buffers instead of forcing GC work whenever the cache grows; benchmarks showed resize thrash when p >= 138M.
- [PerfectNumbers.Core/PrimeCache.cs:26] // TODO: Swap the Open.Numeric enumerator for the staged sieve batches we benchmarked; the iterator allocations here throttle the cache fill when we extend the search past 138M.
- [PerfectNumbers.Core/PrimeCache.cs:28] // TODO: Fold in the Mod6 stride planner that topped Mod6ComparisonBenchmarks so the CPU cache can skip even/composite candidates instead of walking each enumerator element.
- [PerfectNumbers.Core/PrimeCache.cs:93] // TODO: Move this incremental append to the shared sieve batches so we amortize the conversions; walking the enumerator element-by-element is noticeably slower in the updated prime cache benchmarks.
- [PerfectNumbers.Core/PrimesGenerator.cs:55] // TODO: Reuse a single squared local for `p` within this loop so we avoid recalculating `p * p` repeatedly without storing every squared prime globally (the cache must remain limited to the small-divisor tables to honor the memory constraints).
- [PerfectNumbers.Core/RationalHelper.cs:20] // TODO: Cache a reusable byte buffer here so converting to EInteger no longer allocates per call when parsing alpha tables.
- [PerfectNumbers.Core/RleBlacklist.cs:31] // TODO: Replace this HashSet with the pooled ValueHashSet from the blacklist benchmarks so repeated reloads stop allocating new buckets when refreshing the filtered pattern list.
- [PerfectNumbers.Core/StringBuilderPool.cs:19] // TODO: Preserve the builder's capacity when returning it so the pool hands the same buffer back without shrinkage.
- [PerfectNumbers.Core/TextFileWriter.cs:28] // TODO: Surface a lock-free position snapshot (for example via Interlocked.Read on a cached long) so the high-frequency status probes measured in the writer throughput profiling stop contending on the flush lock while runs follow the buffered pipeline planned for the scanner fast path.
- [PerfectNumbers.Core/TextFileWriter.cs:48] // TODO: Use the pooled writer buffers planned for the TextFileWriter fast path so truncation flushes reuse the span caches identified alongside the Mod10_8_5_3Benchmarks instead of forcing synchronous SetLength/Flush work on every restart.
- [PerfectNumbers.Core/TextFileWriter.cs:65] // TODO: Buffer these writes through an ArrayPool-backed accumulator so we only flush when the batch is full; flushing per line shows up as a serialization bottleneck once the scanner emits millions of candidate summaries.
- [PerfectNumbers.Core/TextFileWriter.cs:89] // TODO: Return the writers to a pooled wrapper once the buffered pipeline lands so disposing the TextFileWriter mirrors the zero-allocation strategy validated by the Mod10_8_5_3Benchmarks helpers.

Input/output parsing and formatting modernization
- [EvenPerfectBitScanner.ResultsParser/Program.cs:154] // TODO: Replace TryParse with the Utf8Parser-based span helper once the results parser accepts ReadOnlySpan<char> inputs so numeric option parsing matches the fastest CLI path identified in benchmarks.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:166] // TODO: Switch to the Utf8Parser-based fast path so integer arguments avoid transient strings while parsing.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:192] // TODO: Swap this string slicing for ReadOnlySpan<char>-based parsing once the CLI helpers expose Utf8Parser-compatible overloads so option handling avoids allocating substrings for every argument.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:317] // TODO: Replace ulong.Parse with the Utf8Parser-based span helper so CSV reload stays on the zero-allocation path identified as fastest in the shared CLI benchmarks.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:325] // TODO: Swap bool.Parse for the Utf8Parser-powered boolean reader once exposed so reload avoids allocating temporary strings per record.
- [EvenPerfectBitScanner/Program.cs:110] // TODO: Replace this ulong.Parse with the Utf8Parser-based span helper once CLI option parsing adopts the zero-allocation fast path proven fastest in the parser benchmarks.
- [EvenPerfectBitScanner/Program.cs:125] // TODO: Swap int.Parse for the span-based Utf8Parser fast path to avoid transient strings when processing CLI numeric options.
- [EvenPerfectBitScanner/Program.cs:165] // TODO: Replace ulong.TryParse with the Utf8Parser fast-path once CLI parsing utilities expose span-based helpers for optional arguments.
- [EvenPerfectBitScanner/Program.cs:244] // TODO: Use the Utf8Parser-based reader here to eliminate the temporary substring allocation when parsing warm-up limits.
- [EvenPerfectBitScanner/Program.cs:306] // TODO: Replace this TryParse with the Utf8Parser span helper once shared CLI parsing utilities land, mirroring the other numeric option optimizations.
- [EvenPerfectBitScanner/Program.cs:321] // TODO: Swap to the Utf8Parser double fast-path so zero-hard parsing avoids culture-dependent conversions in the hot CLI path.
- [EvenPerfectBitScanner/Program.cs:337] // TODO: Replace these double/int TryParse calls with span-based Utf8Parser helpers once available so parsing zero-conj stays allocation-free.
- [EvenPerfectBitScanner/Program.cs:371] // TODO: Convert this int.Parse to the Utf8Parser-based helper to keep CLI option parsing allocation-free in the hot startup path.
- [EvenPerfectBitScanner/Program.cs:377] // TODO: Swap int.Parse for Utf8Parser to align with the faster CLI numeric parsing path.
- [EvenPerfectBitScanner/Program.cs:382] // TODO: Replace int.Parse with Utf8Parser-based parsing to eliminate temporary strings when reading slice configuration.
- [EvenPerfectBitScanner/Program.cs:388] // TODO: Use the Utf8Parser-based fast path here to match the other CLI numeric parsing optimizations we plan to adopt.
- [EvenPerfectBitScanner/Program.cs:394] // TODO: Inline the Utf8Parser-based int reader here once shared CLI parsing utilities land so we avoid repeated string allocations.
- [EvenPerfectBitScanner/Program.cs:404] // TODO: Replace int.Parse with the shared Utf8Parser fast-path to keep hot CLI option parsing allocation-free.
- [EvenPerfectBitScanner/Program.cs:418] // TODO: Convert this int.Parse to Utf8Parser once the optimized CLI parsing helper is shared across tools.
- [EvenPerfectBitScanner/Program.cs:848] // TODO: Swap this char-by-char parser for the benchmarked Utf8Parser-based fast-path once the CLI plumbing accepts spans to avoid redundant range checks while collecting primes.
- [EvenPerfectBitScanner/Program.cs:866] // TODO: Swap ulong.TryParse for the Utf8Parser-based fast path once the shared span helpers arrive so by-divisor candidate loading avoids per-value string allocations.
- [EvenPerfectBitScanner/Program.cs:913] // TODO: Replace this ulong.Parse call with the Utf8Parser-based span fast-path we benchmarked so results reload skips the slower string allocation and culture-aware parsing.
- [EvenPerfectBitScanner/Program.cs:933] // TODO: Replace these bool.TryParse calls with the span-based fast path once we expose the Utf8Parser-powered helpers so results reload avoids per-line string allocations for booleans.
- [EvenPerfectBitScanner/Program.cs:1149] // TODO: Switch this StringBuilder-based formatter to a span-friendly Utf8Formatter pipeline so console/file output avoids intermediate builders in the hot logging loop.
- [PerfectNumbers.Core/AlphaCalculations.cs:31] // TODO: Replace the repeated Multiply calls with a span-based aggregator that batches factors, avoiding transient ERational instances when maxR grows large.
- [PerfectNumbers.Core/InputParser.cs:13] // TODO: Replace Split/Parse with the span-based exponent parser so BigInteger inputs avoid allocations and leverage the Utf8Parser fast paths benchmarked for CLI numeric parsing.
- [PerfectNumbers.Core/InputParser.cs:20] // TODO: Replace BigInteger.Parse(string) with the span overload once callers provide ReadOnlySpan<char> so we can route through the allocation-free Utf8Parser-based helpers.
- [PerfectNumbers.Core/InputParser.cs:27] // TODO: Swap decimal.Parse for the Utf8Parser-based decimal reader so callers avoid culture-aware conversions on the hot configuration path.
- [PerfectNumbers.Core/RleBlacklist.cs:247] // TODO: Replace int.TryParse with the Utf8Parser-based span helper so blacklist normalization avoids repeated culture-aware integer parsing in hot loading paths.
- [PerfectNumbers.Core/RleBlacklist.cs:330] // TODO: Swap this int.TryParse for the upcoming Utf8Parser fast path so nested patterns reuse the zero-allocation integer parser during blacklist ingestion.

Benchmark alignment and instrumentation
- [EvenPerfectBitScanner.Benchmarks/GpuUInt128MulModBenchmarks.cs:52] // TODO: Switch the extension method to allocate per iteration, as it's faster. Keep the benchmarks.
- [EvenPerfectBitScanner.Benchmarks/GpuUInt128MulModByLimbBenchmarks.cs:59] // TODO: Switch the extension method back to legacy method, as it's faster. Keep the benchmarks.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:670] // TODO: Replace the separate division/modulo here with Math.DivRem (or the branchless chunk planner from the parallel partition benchmarks) so we avoid paying for two divisions when distributing work across threads.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:894] // TODO: Relocate this limb-based reducer to the benchmark project once the production pipeline switches to the faster allocating legacy path demonstrated in the benchmarks.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:203] BigInteger product = (BigInteger)a * b; // TODO: Replace this BigInteger reduction with the forthcoming UInt128 intrinsic helper measured faster in Mul64Benchmarks and MulHighBenchmarks for huge operands.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:564] d = Gcd128(diff, n, ct); // TODO: Move this to the binary GCD implementation once the UInt128 benchmarks validate the optimized routine.
- [PerfectNumbers.Core/ModResidueTracker.cs:314] // TODO: Replace this double-and-add fallback with the UInt128 intrinsic-backed multiplier once the MulHighBenchmarks-guided implementation lands; the intrinsic path measured dozens of times faster for dense 128-bit workloads.
- [PerfectNumbers.Core/PerfectNumberConstants.cs:14] // TODO: Promote these magic numbers into a runtime profile derived from EvenPerfectBitScanner.Benchmarks so we can retune them automatically when the optimal ranges for 2kp+1 >= 138M change.

Testing backlog
- [EvenPerfectBitScanner/Program.cs:1383] // TODO: Remove this pass-through once callers can provide the full out parameters directly; shaving this hop keeps the hot candidate filter on the shortest path.
- [PerfectNumbers.Core.Tests/AlphaCacheTests.cs:7] [Fact(Skip = "TODO: implement test")]
- [PerfectNumbers.Core.Tests/AlphaCalculationsTests.cs:7] [Fact(Skip = "TODO: implement test")]
- [PerfectNumbers.Core.Tests/InputParserTests.cs:7] [Fact(Skip = "TODO: implement test")]
- [PerfectNumbers.Core.Tests/KRangeFinderTests.cs:7] [Fact(Skip = "TODO: implement test")]
- [PerfectNumbers.Core.Tests/RleBlacklistTests.cs:7] [Fact(Skip = "TODO: implement test")]
- [PerfectNumbers.Core.Tests/TextFileWriterTests.cs:7] [Fact(Skip = "TODO: implement test")]
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:62] // TODO: Inline this shim once callers can invoke OrderOf2ModPrime directly; the extra hop shows up on hot perf traces while we chase large-order factors.

Miscellaneous improvements
- [AGENTS.md:26] > **Temporary workflow notice:** Until the pending apply_patch.cs performance and correctness TODOs are complete, do not invoke the script in this repository. Edit files directly with shell tools (sed, tee, cat, python, etc.) and stage the results via git. Re-enable the helper only after the TODO tracker explicitly calls out that the ban has been lifted.
- [EvenPerfectBitScanner.ResultsParser/Program.cs:537] // TODO: Keep using the Open.Numeric prime enumerator for the merge walk but hoist a shared instance so reloads reuse it instead of allocating a brand new enumerator for every pass.
- [EvenPerfectBitScanner/Program.cs:1228] // TODO: Remove this wrapper once callers can invoke BitOperations.PopCount directly so the hot bit-stat path avoids the extra call layer.
- [PerfectNumbers.Core/AlphaMScannerCandidateStatus.cs:5] // TODO: Store these statuses as byte-coded constants so hot-path evaluations can stay branchless when scanning candidates.
- [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:297] // TODO: Is this expected that the value of kStart is modified?
- [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:317] // TODO: Do we have the check here because we don't support high values or this is expected result?
- [PerfectNumbers.Core/Gpu/GpuKernelPool.cs:465] // TODO: kStart is modified after this. Is this expected?
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:430] // TODO: Review if we can reuse any other existing variables to reduce registry pressure.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1023] // TODO: Can we modify these loops to process multiple bits at a time? E.g. 64-bit chunks.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1131] // TODO: Same as above—migrate callers to the scalar extension to avoid this 6-7× slowdown.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1159] // TODO: This should operate on the instance itself, not on a copy. Avoid creating new instances anywhere.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1162] // TODO: Can we modify these loops to process multiple bits at a time? E.g. 64-bit chunks.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1353] // TODO: Since we ignore the first result element, can we create a version of the function which calculates and returns only the second element?
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1357] // TODO: Why not just modify left instance directly instead using out parameters?
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1440] // TODO: Can we modify this loop to process multiple bits at a time? E.g. 64-bit chunks.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1488] // TODO: Can we modify this loop to process multiple bits at a time? E.g. 64-bit chunks.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1520] // TODO: Can we modify this loop to process multiple bits at a time? E.g. 64-bit chunks.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1621] // TODO: Check if the TODO below is still relevant.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:28] // TODO(NTT-OPT): Consider passing backend explicitly to avoid global state.
- [PerfectNumbers.Core/KRangeFinder.cs:41] // TODO: Collapse this wrapper once the callers can consume the tuple directly; the extra call adds measurable overhead when we sweep millions of Euler primes while scanning the large-division range.
- [PerfectNumbers.Core/KRangeFinder.cs:83] // TODO: Remove this pass-through once the scanning CLI switches to tuple-returning APIs; flattening the call chain saves the extra struct copy flagged in the profiler during the large divisor search.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:56] // TODO: Collapse this wrapper once all callers promote operands to UInt128 so we can jump straight to OrderOf2ModPrime without bouncing through multiple overloads.
- [PerfectNumbers.Core/PrimeTester.cs:14] // TODO: Eliminate this wrapper once callers can reach IsPrimeInternal directly; the extra indirection shows up when we sieve millions of candidates per second.
- [PerfectNumbers.Core/RationalHelper.cs:13] // TODO: Replace the BigInteger-based GCD with the UInt128-friendly binary GCD helper so this rounding path avoids allocating big integers for hot rational conversions.
- [PerfectNumbers.Core/RationalNumbers.cs:8] // TODO: Precompute additional frequently used rationals (e.g., OneHalf, ThreeHalves) so hot alpha computations stop constructing them via ERational.FromInt32 at runtime.
- [PerfectNumbers.Core/StringBuilderPool.cs:12] // TODO: Ensure oversized builders remain eligible for reuse without trimming so call sites keep their full capacity.

Montgomery and Barrett modular arithmetic upgrades
- [EvenPerfectBitScanner.Benchmarks/GpuUInt128Montgomery64Benchmarks.cs:50] // TODO: Migrate remaining callers of MulModMontgomery64 to the extension helper; the struct emulation is ~6.8× slower but kept for GPU parity benchmarks.
- [EvenPerfectBitScanner.Benchmarks/GpuUInt128NativeModuloBenchmarks.cs:49] // TODO: Drop MulModWithNativeModulo from production once all callers use the immediate reduction path; benchmarks show it is 4–8× slower on large 128-bit operands.
- [EvenPerfectBitScanner/Program.cs:69] bool useModuloWorkaround = false; // TODO: Remove once the runtime defaults to the ImmediateModulo path measured fastest in GpuUInt128NativeModuloBenchmarks. removed: useModAutomaton
- [PerfectNumbers.Core/ExponentRemainderStepper.cs:47] // TODO: Route these state resets through the ProcessEightBitWindows helper once the scalar Pow2MontgomeryMod implementation adopts it so delta stepping inherits the benchmarked 2× gains for large exponents.
- [PerfectNumbers.Core/ExponentRemainderStepper.cs:57] // TODO: Once divisor cycle lengths are mandatory, pull the delta multiplier from the single-block divisor-cycle snapshot so we can skip the powmod entirely and reuse the cached Montgomery residue ladder highlighted in MersenneDivisorCycleLengthGpuBenchmarks.
- [PerfectNumbers.Core/ExponentRemainderStepper.cs:87] // TODO: Reuse the divisor-cycle derived Montgomery delta once the cache exposes single-cycle lookups so this branch also avoids recomputing powmods when the snapshot lacks the divisor.
- [PerfectNumbers.Core/ExponentRemainderStepper.cs:124] // TODO: Replace this fallback path with the upcoming ProcessEightBitWindows helper so fresh Montgomery states also benefit from the faster pow2 ladder measured on CPUs.
- [PerfectNumbers.Core/ExponentRemainderStepper.cs:133] // TODO: Once the divisor-cycle cache exposes a direct Montgomery delta, multiply it here instead of relying on the caller-provided delta so incremental scans remain in sync with the snapshot without computing additional cycles or mutating cache state.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:377] // TODO: Pre-reduce the operands via the Montgomery ladder used in MulMod64Benchmarks so the GPU compatible shim stops paying for `%` on every call; the InlineUInt128 helper ran 6–82× faster on large 64-bit workloads while preserving compatibility with the CPU scanner.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:398] // TODO: Fold these operands with the ImmediateModulo helper once the GPU shim exposes it, avoiding repeated `%` reductions that the benchmarks showed are far slower than the Montgomery-based path for dense operands.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:857] // TODO(MOD-OPT): Replace this bitwise long-division style reduction with a faster algorithm suitable for GPU kernels without '%' support. Options: - Montgomery reduction for 128-bit moduli (R=2^128), requires pre- computed modulus-dependent constants (n' and R^2 mod n). This is ideal for NTT primes (odd modulus). Cache per-modulus constants. - Barrett reduction with a 256/128 quotient approximation using only multiplies and shifts. Cache mu = floor(2^k / n) for k=256. Implement a fast path for common 64-bit NTT moduli and a separate path for 128-bit moduli. This will significantly reduce the cost of each butterfly in NTT and LL steps.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:868] // TODO(MOD-OPT): Plumb constants through caches in NttGpuMath.SquareCacheEntry (e.g., Montgomery n', R2) and provide device-friendly accessors.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:986] // TODO: Drop this native-modulo path from production after migrating callers to MulMod, which benchmarked 4-8× faster on dense operands and still wins on mixed workloads.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1111] // TODO: Retire this struct-based Montgomery path from production after adopting the extension helper, which benchmarks 6-7× faster across dense and near-modulus operands.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1209] // TODO: Swap this copy-heavy path for the pooled base/exponent ladder used in GpuUInt128MulModBenchmarks so the GPU shim keeps reusing buffers instead of allocating temporary structs during residue scans.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1215] // TODO: Replace the single-bit square-and-multiply loop with the 64-bit windowed ladder measured fastest in GpuUInt128MulModBenchmarks to align with the GPU kernel implementation once the shared helper lands.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1235] // TODO: Share the pooled ladder state from GpuUInt128MulModBenchmarks here as well so Lucas–Lehmer batches avoid constructing throwaway temporaries on every exponent.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1240] // TODO: Upgrade this loop to the same 64-bit windowed ladder proven fastest in GpuUInt128MulModBenchmarks so the scalar helper matches the GPU-optimized path.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1260] // TODO: Replace this Fermat inversion with the Montgomery ladder highlighted in GpuUInt128Montgomery64Benchmarks so we avoid instantiating a temporary modulus and reuse the pooled reduction constants.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1287] // TODO: Reuse variables to reduce register pressure following the fused-limb layout from GpuUInt128MulModByLimbBenchmarks so the multiply helper matches the fastest GPU-compatible scalar routine.
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1430] // TODO: Can we identify where we deal with Mersenne exponent in EvenPerfectBitScanner and directly use MulModMersenne there, removing this branch?
- [PerfectNumbers.Core/Gpu/GpuUInt128.cs:1623] // TODO(MOD-OPT): Montgomery/Barrett integration plan - Introduce caches of modulus-dependent constants: * Montgomery: n' (-(n^{-1}) mod 2^64 or 2^128), R2 = (R^2 mod n) * Barrett: mu = floor(2^k / n) for k ∈ {128, 192, 256} - Add fast-path for 64-bit NTT primes (modulus.High == 0UL) using pure 64-bit ops. - Expose helpers to retrieve/calc constants once per modulus and reuse in kernels. - Wire these into MulMod and SquareMod hot paths under feature toggles. - Ensure ILGPU compatibility (no BigInteger, no % inside kernels).
- [PerfectNumbers.Core/Gpu/MersenneNumberLucasLehmerGpuTester.cs:493] // TODO: Swap this UInt128 `%` reduction for the GPU-compatible MulMod helper once it adopts the faster inline UInt128 path benchmarked in MulMod64Benchmarks so host/GPU parity avoids BigInteger-style fallbacks.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:35] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, int, int, int, ArrayView<GpuUInt128>, ulong, ulong>> StageMontKernelCache = new(); // TODO: Inline the Montgomery kernels into the startup table instead of using a concurrent cache now that we no longer mutate state at runtime.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:36] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, int, int, int, ArrayView<GpuUInt128>, ulong, ulong, ulong, ulong>> StageBarrett128KernelCache = new(); // TODO: Same plan – reuse the precomputed kernels measured fastest in MontgomeryMultiplyBenchmarks rather than looking them up dynamically.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:37] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, GpuUInt128, ulong, ulong, ulong, ulong>> ScaleBarrett128KernelCache = new(); // TODO: Collapse into the startup kernel array once Barrett128 constants are preloaded.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:38] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, ulong, ulong, ulong, ulong>> SquareBarrett128KernelCache = new(); // TODO: Fold these kernels into the same startup table to avoid concurrent access overhead highlighted in GpuModularArithmeticBenchmarks.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:39] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, ulong, ulong, ulong>> ToMont64KernelCache = new(); // TODO: Prebind and reuse the Montgomery conversions from MontgomeryMultiplyBenchmarks instead of storing them in a concurrent cache.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:42] private static readonly ConcurrentDictionary<Accelerator, Action<Index1D, ArrayView<GpuUInt128>, ulong, ulong, ulong>> ScaleMont64KernelCache = new(); // TODO: Move to the startup kernel table so Montgomery-scaled launches skip concurrent lookups.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:60] // TODO(MOD-OPT): Precompute and store Montgomery/Barrett constants for modulus here (e.g., Montgomery n', R2; Barrett mu) to avoid recomputation.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:114] // TODO: Pull these stage roots from the precomputed tables measured fastest in Pow2MontgomeryModCycleComputationBenchmarks instead of recomputing ModPow for every stage.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:125] // TODO: Reuse the same precomputed tables for inverse roots so we do not repeat the expensive ModPow calls highlighted as bottlenecks in Pow2MontgomeryModCycleComputationBenchmarks.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:148] // TODO: Preload MontNPrime64/MontR values using the MontgomeryMultiplyBenchmarks helper so startup reuses the fastest constant generation routine instead of recomputing here.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:547] // TODO(MOD-OPT): Stage kernel using 64-bit Montgomery multiplication. Requires data and twiddles in Montgomery domain.
- [PerfectNumbers.Core/Gpu/NttGpuMath.cs:752] // TODO(NTT-OPT): Replace this reference O(n^2) kernel with a stage-wise Cooley–Tukey butterfly kernel (O(n log n)). This version uses a per- element ModPow in the hot loop which is far too slow and causes long single-kernel runtimes. After twiddle precomputation, each butterfly should perform: (u, v) -> (u+v*w, u-v*w) with a single MulMod.
- [PerfectNumbers.Core/MersenneNumberTester.cs:39] // TODO: Swap this ConcurrentDictionary for the pooled dictionary variant highlighted in Pow2MontgomeryModBenchmarks once order warmups reuse deterministic divisor-cycle snapshots; the pooled approach removed the locking overhead when scanning p >= 138M in those benchmarks.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:134] // TODO: Replace this duplicate powmod with ULongExtensions.ModPow64 so we inherit the optimized MulMod64 path that led the MulMod64Benchmarks on large operands.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:166] private static ulong MulMod64(ulong a, ulong b, ulong mod) => (ulong)((UInt128)a * b % mod); // TODO: Route this through ULongExtensions.MulMod64 so the powmod above adopts the inline UInt128 implementation that dominated the MulMod64Benchmarks.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:169] // TODO: Switch to UInt128Extensions.ModPow once its MulMod backend adopts the faster UInt128BuiltIn path highlighted in MulHighBenchmarks to avoid the BigInteger fallback.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:394] x = (MulMod64(x, x, n) + c) % n; // TODO: Swap the `% n` with the Montgomery folding helper from the PollardRho benchmarks once the specialized reducer lands.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:402] y = (MulMod64(y, y, n) + c) % n; // TODO: Use the same Montgomery folding helper here to keep the tortoise sequence on the optimized path.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:410] y = (MulMod64(y, y, n) + c) % n; // TODO: Replace this modulo with the optimized helper so both steps share the fast reduction.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:448] // TODO: Port this Miller–Rabin routine to UInt128 intrinsics so we avoid the BigInteger.ModPow calls that lag by orders of magnitude on the large inputs highlighted in the Pow2MontgomeryMod benchmarks.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:542] x = (MulMod128(x, x, n) + c) % n; // TODO: Swap this modulo for the UInt128 Montgomery reducer once the optimized helper lands.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:549] y = (MulMod128(y, y, n) + c) % n; // TODO: Use the same optimized reducer for the tortoise step.
- [PerfectNumbers.Core/MersennePrimeFactorTester.cs:556] y = (MulMod128(y, y, n) + c) % n; // TODO: Replace with the optimized reducer so both steps avoid BigInteger-based modulo.
- [PerfectNumbers.Core/ModResidueTracker.cs:257] // TODO: Use the Montgomery folding helper measured faster in the Pow2Montgomery benchmarks so this wide residue path avoids general `%` on UInt128 inputs.
- [PerfectNumbers.Core/ModResidueTracker.cs:295] // TODO: Route this through the planned UInt128 windowed powmod so the inner MulMod adopts the UInt128BuiltIn path that dominated the MulHighBenchmarks on wide operands instead of the current square-and-multiply loop.
- [PerfectNumbers.Core/PowerCache.cs:16] // TODO: Rent this buffer from ArrayPool<BigInteger> (or a dedicated pooled cache) so expanding exponent ranges stop allocating fresh arrays as we sweep across large divisor sets in the Pow2MontgomeryModBenchmarks scenarios.
- [PerfectNumbers.Core/PowerCache.cs:42] // TODO: Swap this BigInteger chain with the UInt128-based Montgomery ladder from Pow2MontgomeryModBenchmarks once callers guarantee 64-bit inputs; the benchmarks show the arbitrary-precision multiply is orders of magnitude slower for the exponents we scan when p >= 138M.
- [PerfectNumbers.Core/UInt128Extensions.cs:70] // TODO: Prototype a UInt128-native Pow2MontgomeryModWindowed path that matches the GPU implementation once a faster modular multiplication helper is available, so we can revisit removing the GPU struct dependency without regressing benchmarks.
- [PerfectNumbers.Core/ULongExtensions.cs:276] // TODO: Replace this `%` with the Montgomery folding helper highlighted in MulMod64Benchmarks so the modular exponentiation avoids the slow integer division before the ladder even starts.
- [done] [PerfectNumbers.Core/ULongExtensions.cs:1074] // TODO: Wire this cycle-aware overload into the ProcessEightBitWindows helper so the reduced exponent path inherits the faster windowed pow2 routine highlighted in the Pow2Montgomery benchmarks. Return 1 because 2^0 = 1
- [PerfectNumbers.Core/ULongExtensions.cs:520] // TODO: Replace this modulo with the cached cycle remainder produced by the divisor-cycle cache so Pow2ModWindowed avoids repeated `%` work, matching the ProcessEightBitWindows wins captured in Pow2MontgomeryModCycleComputationBenchmarks.
- [PerfectNumbers.Core/ULongExtensions.cs:536] // TODO: Replace this modulo with the cached cycle remainder produced by the divisor-cycle cache so Pow2ModWindowed avoids repeated `%` work, matching the ProcessEightBitWindows wins captured in Pow2MontgomeryModCycleComputationBenchmarks.
- [done] [PerfectNumbers.Core/ULongExtensions.cs:1219] // TODO: Swap this modulo with the shared UInt128 cycle remainder helper once available so CRT powmods reuse cached reductions in the windowed ladder, avoiding the `%` cost highlighted in Pow2MontgomeryModCycleComputationBenchmarks.

Windowed exponentiation and ProcessEightBitWindows adoption
- [PerfectNumbers.Core/Gpu/GpuConstants.cs:7] // TODO: Auto-tune these GPU constants per accelerator so batch sizing aligns with the fastest kernel configuration reported by the Pow2Mod and divisor-cycle benchmarks.
